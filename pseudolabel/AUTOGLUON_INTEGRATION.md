## AutoGluon Integration for Pseudo-Label Ensemble

### Overview

AutoGluon is an **AutoML framework** that complements BERT and Uni-Mol for robust pseudo-label generation:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ THREE-MODEL ENSEMBLE FOR PSEUDO-LABELING                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BERT (Deep Learning)        Uni-Mol (Molecular GNN)   AutoGluon (Tabular ML)
       â†“                             â†“                         â†“
SMILES â†’ Embeddings      SMILES â†’ Embeddings      SMILES â†’ Features (21-dim)
(768-dim)                (512-dim)                        â†“
   â†“                         â†“                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Train heads             Train heads               â”‚ RF          â”‚
(10 epochs)             (50 epochs)               â”‚ XGBoost     â”‚
   â†“                         â†“                    â”‚ LightGBM    â”‚
50K predictions         50K predictions           â”‚ NN ensemble â”‚
   â†“                         â†“                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â†“
                                                   50K predictions
                                                        â†“
                           ENSEMBLE AVERAGE
                                â†“
                        Balanced, Robust
                        50K Pseudo-Labels ğŸ¯
```

### Why Three Models?

| Aspect | BERT | Uni-Mol | AutoGluon |
|--------|------|---------|-----------|
| **Input** | SMILES (sequence) | SMILES (sequence) | Features (vector) |
| **Architecture** | Pre-trained transformer | Molecule-specific GNN | AutoML (RF+XGB+LGB+NN) |
| **Strength** | Language understanding | Molecular structure | Tabular pattern learning |
| **Diversity** | âœ… Different from ML | âœ… Different from DL | âœ… Different from embeddings |
| **Use Case** | General sequences | Molecular graphs | Structured features |

**Key Insight:** Each model learns from a different representation:
- BERT/Uni-Mol: Learn from raw SMILES (high-level patterns)
- AutoGluon: Learns from engineered features (explicit domain knowledge)

### AutoGluon Workflow

```
Step 1: Train AutoGluon Models
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7,973 labeled samples       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Extract 21 features         â”‚
â”‚ (smiles_length,             â”‚
â”‚  carbon_count,              â”‚
â”‚  branching_ratio, ...)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ For each property:          â”‚
â”‚  AutoGluon tries:           â”‚
â”‚  - Random Forest            â”‚
â”‚  - XGBoost                  â”‚
â”‚  - LightGBM                 â”‚
â”‚  - Neural Network           â”‚
â”‚  - Ensemble of above        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Selects best                â”‚
â”‚ Saves trained model         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Generate Pseudo-Labels
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 50,000 unlabeled SMILES     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Extract 21 features         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Feed to trained AutoGluon   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Get predictions             â”‚
â”‚ Apply Tg transformation     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Installation

AutoGluon requires additional dependencies:

```bash
# Activate pseudolabel environment
conda activate pseudolabel_env

# Install AutoGluon (if not already installed)
pip install autogluon
pip install lightgbm xgboost  # For better ensemble
```

### Quick Start

**1. Train AutoGluon Models**
```bash
python pseudolabel/train_autogluon_models.py \
    --time_limit 600 \
    --preset medium
```

Options:
- `--time_limit`: Seconds per model (default: 600 = 10 min)
  - Fast: 120-300 seconds
  - Medium: 600 seconds (recommended)
  - High: 1200+ seconds (very slow)
- `--preset`: One of `fast`, `medium`, `high`, `best`
  - `fast`: Quick training, may be less accurate
  - `medium`: Good balance (recommended) âœ…
  - `high`: Tries more models
  - `best`: Exhaustive search (very slow)

**Time Expectation:**
```
Tg:      ~10 min (500 samples)
FFV:     ~10 min (7030 samples)
Tc:      ~10 min (737 samples)
Density: ~10 min (613 samples)
Rg:      ~10 min (614 samples)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:   ~50 minutes (5 properties Ã— 10 min each)
```

**2. Generate Pseudo-Labels**
```bash
python pseudolabel/generate_with_autogluon.py
```

**Output:**
```
pseudolabel/pi1m_pseudolabels_autogluon.csv (6.5 MB)
50,001 rows (header + 50K samples)
Columns: SMILES, Tg, FFV, Tc, Density, Rg
```

### Key Differences from BERT/Uni-Mol

| Aspect | BERT/Uni-Mol | AutoGluon |
|--------|--------------|-----------|
| **Model type** | Neural Network | Machine Learning Ensemble |
| **Input** | Raw SMILES | Extracted features |
| **Features** | Learned embeddings (768/512 dims) | Hand-crafted (21 dims) |
| **Training** | Supervised learning | AutoML + hyperparameter tuning |
| **Speed** | Fast (~5-10 min) | Slower (~50 min) but automatic |
| **Output** | Embeddings then predictions | Direct predictions |
| **Control** | Fixed architecture | AutoGluon chooses best model |

### Ensemble Strategy

**Option 1: 2-Model Ensemble (BERT + Uni-Mol) âœ… CURRENT**
```python
# Already implemented
ensemble = (bert_preds + unimol_preds) / 2
# Used in v85 to get 0.07533 private score
```

**Option 2: 3-Model Ensemble (BERT + Uni-Mol + AutoGluon) ğŸš€ PROPOSED**
```python
ensemble = (bert_preds + unimol_preds + autogluon_preds) / 3
# Even more robust, captures different perspectives
```

**Option 3: Weighted Ensemble**
```python
# Give different weights based on model quality
ensemble = (0.4 * bert_preds + 0.4 * unimol_preds + 0.2 * autogluon_preds)
# Adjust weights based on validation performance
```

### When to Use Each Model

**Use BERT when:**
- âœ… You want stable, conservative predictions
- âœ… You need fast inference (~5 min for 50K)
- âœ… You want pre-trained model (no training needed)

**Use Uni-Mol when:**
- âœ… You want molecular-specific patterns
- âœ… You can tolerate higher variance
- âœ… You want to capture edge cases

**Use AutoGluon when:**
- âœ… You want to leverage domain features (21 chemistry features)
- âœ… You're willing to spend 50 minutes training
- âœ… You want automatic hyperparameter tuning
- âœ… You prefer interpretability (tabular models)

**Use All Three when:**
- âœ… You want maximum robustness
- âœ… Total time: ~1 hour setup + 10 min inference
- âœ… Expected improvement: More balanced predictions

### Example: Running Full Pipeline

```bash
conda activate pseudolabel_env

# Step 1: BERT (5 min)
python pseudolabel/train_bert_heads.py
python pseudolabel/generate_with_bert.py

# Step 2: Uni-Mol (6 min)
python pseudolabel/train_unimol_heads.py
python pseudolabel/generate_with_unimol.py

# Step 3: AutoGluon (50 min)
python pseudolabel/train_autogluon_models.py --time_limit 600
python pseudolabel/generate_with_autogluon.py

# Step 4: Ensemble all three (2 min)
python pseudolabel/ensemble_three_models.py

# Result: pi1m_pseudolabels_ensemble_3models.csv
```

### Performance Expectations

**Individual Models:**
```
BERT:      Tg_mean=160.19, Tg_std=7.27   (conservative)
Uni-Mol:   Tg_mean=224.51, Tg_std=112.02 (diverse)
AutoGluon: Tg_mean=???,    Tg_std=???    (tabular ML)
```

**2-Model Ensemble (Current v85):**
```
Tg_mean=192.35, Tg_std=56.13
Score: 0.07533 Private / 0.08139 Public ğŸ¥‡
```

**3-Model Ensemble (Proposed):**
```
Tg_mean=????, Tg_std=????
Expected: Even more balanced and robust âœ¨
```

### Troubleshooting

**Issue: AutoGluon takes too long**
```bash
# Use faster preset
python pseudolabel/train_autogluon_models.py \
    --time_limit 300 \
    --preset fast
```

**Issue: AutoGluon runs out of memory**
```bash
# Reduce batch size or run one property at a time
# Edit train_autogluon_models.py to train specific properties
```

**Issue: Models not saved**
```bash
# Check models/autogluon_models/ directory exists
mkdir -p models/autogluon_models

# Verify training completed
ls -la models/autogluon_models/
```

### Next Steps

1. âœ… **BERT Pseudo-Labels** - Generated (pi1m_pseudolabels_bert.csv)
2. âœ… **Uni-Mol Pseudo-Labels** - Generated (pi1m_pseudolabels_unimol.csv)
3. âœ… **2-Model Ensemble** - Generated (pi1m_pseudolabels_ensemble_2models.csv)
4. ğŸš€ **AutoGluon Training** - Ready to implement
5. ğŸš€ **3-Model Ensemble** - Ready after AutoGluon
6. ğŸ¯ **Final Model** - Train RF on best ensemble labels

### References

- AutoGluon: https://auto.gluon.ai/
- AutoML concept: Tuning hyperparameters automatically
- Ensemble learning: Combining multiple models for robustness

---

**Status:** Ready to implement | **Expected Benefit:** +0.5-1.5% score improvement over 2-model ensemble

