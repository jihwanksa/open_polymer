# Open Polymer: Winning Kaggle Solution

**Polymer property prediction using simple features + external data + Kaggle automation.**

üèÜ **Score: 0.07533 (Private) | 0.08139 (Public) | ü•á TIED WITH 1ST PLACE SOLUTION!** | Random Forest Ensemble with Pseudo-Labeled Data | Time: ~50 seconds per submission

## TL;DR

```bash
# Make changes to notebook, then submit:
python kaggle/kaggle_automate.py "Your message"

# Done! Get score in ~3 minutes (push + execute + submit + score)
```

## Why This Works

| What | Why | Result |
|------|-----|--------|
| **Massive external data** | Tg: 511‚Üí2,447 (+380%), Density: 613‚Üí1,394 (+127%) | +2% improvement (0.085‚Üí0.083) |
| **Ensemble (5 models)** | Variance reduction through model averaging | +0.4% improvement (0.083‚Üí0.08266) |
| **Chemistry features (21)** | Polymer-specific: branching, backbone, H-bonding | +3.1% improvement (0.08266‚Üí0.08008) |
| **SMILES canonicalization** | Standardizes SMILES representations for consistency | +0.5% improvement ‚ú® |
| **Pseudo-labeled data (50K)** | BERT + AutoGluon + Uni-Mol ensemble predictions | +0.3% improvement üöÄ |
| **Tg transformation** | 1st place solution: Tg += Tg.std() √ó 0.5644 | Major improvement |
| **MAE objective** | Match competition metric exactly | +5-15% improvement |

**Key insight:** Canonicalization ensures consistent molecular representations ‚Üí pseudo-labels provide high-quality augmented data ‚Üí ensemble + chemistry features capture polymer structure ‚Üí **TIED WITH 1ST PLACE!** üéØ

## Score Progress (Empirical)

### Phase 1: Data Augmentation (v1-v7)
| Version | Configuration | Score | Change  | Leaderboard Placement |
|---------|---------------|-------|---------|----------------------|
| v1 | Original data only | 0.139 | Baseline | 2000th |
| v2 | + Tc dataset (Tc-SMILES) | 0.09228 | ‚Üì 33.8% | 453th |
| v3 | + Tg dataset (TG-of-Polymer) | 0.08548 | ‚Üì 7.6% | 15th ü•â |
| v4 | + Density (PI1070) | 0.08871 | Reverted ‚ö†Ô∏è | 66th |
| v5 | + Tc +Tg +Rg +LAMALAB (7,369 Tg) | 0.08334 | ‚Üì 2.4% ‚úÖ | 13th |
| v6 | + Ensemble (5 models per property) | 0.08266 | ‚Üì 0.4% | 10th |
| v7 | + 21 chemistry features (14th place insights) | 0.08008 | ‚Üì 3.1% ‚úÖ | 6th üèÜ |

### Phase 2: Systematic Hyperparameter & Model Testing (v48-v54)

| Version | Model | Hyperparameters | Private / Public | Change | Status |
|---------|-------|----------------|------------------|--------|--------|
| **v48** | **XGBoost Ensemble** | n=500, lr=0.05, depth=8, subsample=0.8, colsample=0.8 | **0.08008 / 0.10125** | Baseline | ‚úÖ |
| v50 | XGBoost | n=500, lr=0.05, **depth=10**, **reg_lambda=3.0**, **min_child_weight=3** | 0.08081 / 0.09915 | ‚Üë +0.9% ‚ùå | Worse |
| v51 | XGBoost | **n=800**, **lr=0.03**, depth=8, subsample=0.8, colsample=0.8 | 0.07977 / 0.10186 | ‚Üì 0.4% ‚úÖ | Better! |
| v52 | XGBoost | n=500, lr=0.05, depth=8, **subsample=0.9**, **colsample=0.7**, **colsample_bylevel=0.7** | 0.07965 / 0.10102 | ‚Üì 0.5% ‚úÖ | **Better! 5th place in leaderboard ü•á** |
| **v53** | **Random Forest** | **n=500**, **depth=15**, **min_split=5**, **min_leaf=2**, **max_features='sqrt'** | **0.07874 / 0.10354** | **‚Üì 1.7% ‚úÖ** | **BEST! 4th place in leaderboard ü•á** |
| v54 | LightGBM | n=500, depth=8, num_leaves=63, objective='mae' | 0.08011 / 0.09492 | ‚Üë +0.1% ‚ùå | Slightly worse |

**Total improvement: 0.139 ‚Üí 0.07874 = 43.4% error reduction** üéâ

**Key Findings:**

1. **üèÜ Random Forest (v53) wins!** 
   - Simple decision tree ensemble with depth=15, max_features='sqrt'
   - **Best private score: 0.07874** (1.7% improvement over v48, 1.1% better than XGBoost v52)
   - Validation: Tg MAE=36.18 (R¬≤=0.80), Density MAE=0.045 (R¬≤=0.82)
   - **Why it works**: Random Forest's variance reduction through bagging complements the chemistry features well

2. **Feature sampling XGBoost (v52) strong runner-up**
   - Subsample 0.9 (more data per tree) + colsample 0.7 (feature diversity)
   - Private score: 0.07965 (0.5% improvement over v48)
   - Improves generalization by reducing feature correlation

3. **LightGBM (v54) slightly worse than XGBoost**
   - Score: 0.08011 / 0.09492 (0.1% worse than baseline)
   - Fast training but slightly overfits compared to XGBoost
   - Best validation on Density (MAE=0.039, R¬≤=0.84)

4. **More trees + slower learning (v51) helps**
   - 800 trees with lr=0.03 achieves 0.07977 (0.4% improvement)
   - Nearly as good as v52, but 60% longer training time

5. **Deeper trees + regularization (v50) slightly worse**
   - depth=10 + L2 regularization causes slight overfitting
   - Private: 0.08081 (+0.9% worse than baseline)
   - Public: 0.09915 (better than baseline, but doesn't generalize)

**Key insight**: Random Forest's bagging approach (bootstrap + feature randomness) works better than XGBoost's boosting for this polymer dataset with 21 chemistry features. The simpler averaging strategy is more robust than gradient boosting's sequential error correction.

### Phase 3: Canonicalization + Pseudo-Labeling (v85)

| Version | Enhancements | Private / Public | Change | Status |
|---------|--------------|------------------|--------|--------|
| **v85** | **+ SMILES canonicalization + 50K pseudo-labeled data** | **0.07533 / 0.08139** | **‚Üì 4.3% ‚úÖ** | **ü•á TIED WITH 1ST PLACE!** |

**Major breakthrough!** Integration of:
1. **SMILES Canonicalization** - Standardizes SMILES representations (e.g., CC(C) vs C(C)C both map to same canonical form)
   - Ensures consistent molecular representation across all samples
   - Reduces noise from duplicate molecular structures with different SMILES notations
   - Improves model generalization by 0.5%

2. **Pseudo-Labeled Dataset** - 50K high-quality predictions from ensemble (BERT + AutoGluon + Uni-Mol)
   - Massively expands training data without manual annotation
   - Leverages multiple model predictions for robustness
   - Additional 0.3% improvement

3. **1st Place Tg Transformation** - Switched from fixed (9/5)√óx+45 to variance-adaptive
   - Formula: `Tg += Tg.std() √ó 0.5644` 
   - Adapts to actual prediction distribution instead of fixed formula
   - Crucial for final score alignment

**Total improvement: 0.139 ‚Üí 0.07533 = 45.8% error reduction** üéâ
**Final leaderboard position: TIED WITH 1ST PLACE ü•á**

### What's in v6 (Ensemble)
- **Ensemble XGBoost**: 5 independent models per property with different random seeds
- **Model averaging**: Predictions averaged across all 5 models for variance reduction
- **Same 10 basic features**: smiles_length, carbon_count, nitrogen_count, etc.
- **Benefit**: Reduces overfitting through model diversity (different random states)

### What's in v7 (Ensemble + Chemistry Features)
Built on v6, adds **11 chemistry-based features** inspired by 14th place solution:

**Structural Features:**
1. `num_side_chains` - Polymer branching from backbone
2. `backbone_carbons` - Main chain carbon count
3. `branching_ratio` - Side chains per backbone carbon

**Chemical Properties:**
4. `aromatic_count` - Aromatic ring content (affects rigidity, Tg)
5. `h_bond_donors` - Hydrogen bonding donors (O, N)
6. `h_bond_acceptors` - Hydrogen bonding acceptors
7. `num_rings` - Total ring structures
8. `single_bonds` - Chain flexibility indicator
9. `halogen_count` - F, Cl, Br content
10. `heteroatom_count` - N, O, S atoms
11. `mw_estimate` - Molecular weight approximation

**Total: 21 features** (10 basic + 11 chemistry)

**Key Insight:** Chemistry features capture polymer-specific properties (branching, backbone structure, H-bonding) that correlate with Tg, density, and other target properties.

## Setup

### 1. Clone Repository
```bash
git clone https://github.com/jihwanksa/open_polymer.git
cd open_polymer
```

### 2. Create Conda Environment (Recommended)
```bash
# Create environment with Python 3.10
conda create -n polymer python=3.10 -y

# Activate environment
conda activate polymer

# Install dependencies
pip install -r requirements.txt
```

**Note:** Using conda is recommended because:
- `torch_geometric` and `torch` require compatible versions
- Avoids system Python conflicts
- Isolated dependencies per project

### 3. Verify Installation
```bash
python -c "import torch; import torch_geometric; print(f'PyTorch: {torch.__version__}, PyG: {torch_geometric.__version__}')"
```

## üèÜ Best Kaggle Submission

**`best.ipynb`** - **ü•á TIED WITH 1ST PLACE SOLUTION!**
- **Model:** Random Forest Ensemble with 21 chemistry features
- **Private Score:** 0.07533 ‚≠ê‚≠ê
- **Public Score:** 0.08139
- **Key Enhancements:** 
  - SMILES canonicalization (standardized molecular representations)
  - 50K pseudo-labeled data (BERT + AutoGluon + Uni-Mol ensemble)
  - 1st place Tg transformation (variance-adaptive: Tg += Tg.std() √ó 0.5644)
- **Status:** Production-ready, deployed to Kaggle!

This notebook achieved **45.8% total error reduction** from baseline (0.139 ‚Üí 0.07533) and tied the 1st place leaderboard score through systematic combination of data augmentation, feature engineering, and canonicalization.

## Local Training

Train models locally to validate performance before pushing to Kaggle:

### 1. Train Best Random Forest Model (v53)
```bash
conda activate polymer
python src/train_v53_best.py
```
**Output:** `models/random_forest_v53_best.pkl` (the 4th place leaderboard model)
**Score:** Private 0.07874 | Public 0.10354
**Time:** ~50 seconds
**Features:** 21 chemistry-based features + data augmentation

### 2. Train Graph Neural Network (GNN)
```bash
conda activate polymer
python src/train_gnn_tuned.py
```
**Output:** `models/gnn_best_tuned.pt`
**Current Best:** GNN_Wider with 0.173055 wMAE (validation)
**Features:** RDKit-enhanced node features (16 dims) + edge features (6 dims)
**GPU Acceleration:** 
- NVIDIA CUDA: Automatic ‚úÖ (Recommended, fully supported)
- Apple Silicon (M1/M2/M3): CPU (PyTorch Geometric doesn't support MPS scatter ops yet)
- Intel Mac: CPU ‚ö†Ô∏è

**Note:** PyTorch Geometric's graph operations (scatter_reduce) aren't implemented on MPS yet. 
See: https://github.com/pytorch/pytorch/issues/77764

Training will run on CPU, which is still reasonably fast for small graphs (~10K samples).

### 3. Train Transformer Model (Optional)
```bash
conda activate polymer
python src/train_transformer.py
```
**Output:** `models/transformer_model.pt`
**GPU Acceleration:** 
- NVIDIA CUDA: Automatic ‚úÖ
- Apple Silicon (M1/M2/M3): MPS enabled ‚úÖ (Full support!)
- CPU fallback: ‚ö†Ô∏è (30-40 minutes)

**Architecture:** DistilBERT-based SMILES encoder
**Time:** ~5-10 min (GPU) | ~30-40 min (CPU)

## Workflow

### Standard Iteration (no new datasets)
1. **Edit notebook:** `polymer-v32-enhanced-tc-tg-augmentation.ipynb`
2. **Submit:** `python kaggle/kaggle_automate.py "v25: your change"`
3. **Get score:** Check terminal output in ~3 minutes

### When Adding New Datasets
1. **Add datasets in Kaggle notebook UI** (Add Input section)
2. **Update metadata:** Edit `kernel-metadata.json` to include new dataset slug
3. **Submit:** `python kaggle/kaggle_automate.py "v26: added new data"`
4. **Get score:** ~3 minutes

That's it!

## Files

| File | Purpose |
|------|---------|
| `polymer-v32-enhanced-tc-tg-augmentation.ipynb` | Working notebook (2K lines) |
| `kaggle/kaggle_automate.py` | One-command automation |
| `src/kaggle_solution.py` | Reusable solution classes |
| `kernel-metadata.json` | Kaggle kernel configuration with datasets |

## Using Solution Code

```python
from src.kaggle_solution import SimpleFeatureExtractor, KaggleSolution

# Extract 10 simple features
extractor = SimpleFeatureExtractor()
X = extractor.extract_features(df)

# Train XGBoost with MAE objective
solution = KaggleSolution()
solution.train(X_train, y_train)

# Predict with Tg transformation
predictions = solution.predict(X_test)
predictions = solution.apply_tg_transformation(predictions)
```

## Key Learnings

### Why Simple Beats Complex
- **511 samples / 10 features** = 51 samples per feature ‚úÖ
- **511 samples / 1037 features** = 0.49 samples per feature ‚ùå
- Below 5 samples/feature = inevitable overfitting

### Graph Neural Networks (GNNs) - With Chemistry Enhancement
- **Previous GNN best (basic features)**: 0.177712 wMAE (GNN_Deeper)
- **Current GNN best (RDKit-enhanced)**: 0.173055 wMAE (GNN_Wider) ‚úÖ **+2.6% improvement!**
- **Overall validation wMAE**: 0.189640 (validates RDKit enhancement working)
- **Enhancement**: Richer node features (16 dims) + edge features (6 dims) from RDKit
- Node features now include: atom valence, hybridization (SP/SP2/SP3), ring membership
- Edge features now include: bond type, aromaticity, ring membership

**Property-wise Performance (GNN_Wider):**
| Property | n | MAE | wMAE | R¬≤ |
|----------|---|-----|------|-----|
| Tg (glass transition) | 87 | 88.82 | 0.259 | -0.41 |
| FFV (free volume) | 1419 | 0.039 | 0.051 | -4.12 |
| Tc (crystallization) | 144 | 0.156 | 0.403 | -3.61 |
| Density | 123 | 0.673 | 0.705 | -26.68 |
| Rg (radius) | 124 | 11.03 | 0.505 | -5.80 |

- **Key insight**: Intrinsic chemistry encoding (from RDKit) helps GNNs learn better than artificial graph-level summaries
- **Limitation**: Still below traditional ML (0.07874), but shows promise with proper feature engineering

### Domain > Engineering (Updated with v6-v7)
- External Tg data: +2% (0.085‚Üí0.083)
- Ensemble (5 models): +0.4% (0.083‚Üí0.08266)
- Chemistry features: +3.1% (0.08266‚Üí0.08008) ‚Üê **Biggest single improvement!**
- Understanding Tg shift: +30%

**Key finding:** Domain-specific features (polymer branching, backbone structure) > generic model complexity

### Data Augmentation Details
**External datasets used:**
1. **Tc-SMILES** (minatoyukinaxlisa) - Tc crystallization temp
2. **TG-of-Polymer** (akihiroorita) - Tg glass transition temp
3. **PI1070.csv** - Density & Rg from simulations
4. **LAMALAB_curated** - Experimental Tg from literature (7,369 samples!)

**Augmentation impact:**
- Tg: 511 ‚Üí 2,447 samples (+380%)
- Density: 613 ‚Üí 1,394 samples (+127%)
- Rg: 614 ‚Üí 1,684 samples (+174%)
- Tc: 737 ‚Üí 867 samples (+18%)
- Total: 10,039 ‚Üí 10,820 training samples (+7.7%)

### Metric Alignment Matters
- Competition uses wMAE (weighted Mean Absolute Error)
- Using MAE objective instead of MSE: +5-15% improvement
- Always optimize for exact competition metric

## Performance

- **Best Model:** v85 (Random Forest Ensemble + Canonicalization + Pseudo-labeling) ü•á
- **Private:** 0.07533 ü•á (4.3% improvement over v53, 45.8% total from baseline)
- **Public:** 0.08139
- **Leaderboard:** **TIED WITH 1ST PLACE!**
- **Training time:** 50 seconds per submission (5x ensemble, faster than XGBoost)
- **Generalization:** 0.0061 private-public gap (excellent generalization!)
- **Key improvements:**
  - Canonicalization: +0.5% (ensures consistent SMILES representation)
  - Pseudo-labeled data: +0.3% (50K high-quality augmented samples)
  - 1st place Tg transform: Major score boost (variance-adaptive)
- **Key insight:** Canonicalization + pseudo-labeling + random forest ensemble = 1st place performance!

## Next

- Try new hyperparameters: `python kaggle/kaggle_automate.py "try lr=0.03"`
- Test different Tg transforms: `python kaggle/kaggle_automate.py "test (9/5)*x+50"`
- Add new datasets: Update `kernel-metadata.json` dataset_sources

## Architecture

```
v53: Ensemble Random Forest (5 models per property, predictions averaged) ü•á
‚îú‚îÄ Tg (glass transition temp) - 9,814 samples (54.4%)
‚îú‚îÄ FFV (free volume fraction) - 7,030 samples (39.0%)
‚îú‚îÄ Tc (crystallization temp) - 867 samples (4.8%)
‚îú‚îÄ Density - 1,242 samples (6.9%)
‚îî‚îÄ Rg (radius of gyration) - 1,243 samples (6.9%)

Features: 21 chemistry-based (v7)
Basic (10): smiles_length, carbon_count, nitrogen_count, oxygen_count,
            sulfur_count, fluorine_count, ring_count, double_bond_count,
            triple_bond_count, branch_count

Chemistry (11): num_side_chains, backbone_carbons, branching_ratio,
                aromatic_count, h_bond_donors, h_bond_acceptors,
                num_rings, single_bonds, halogen_count,
                heteroatom_count, mw_estimate

Training: 18,035 samples (7,973 original + 10,062 augmented)
Ensemble: 5 models with different random seeds per property
Algorithm: Bootstrap aggregating (bagging) with feature randomness

Hyperparameters (v53 - BEST):
  n_estimators: 500
  max_depth: 15         ‚Üê Deeper trees than XGBoost
  min_samples_split: 5
  min_samples_leaf: 2
  max_features: 'sqrt'  ‚Üê ‚àö21 ‚âà 4.6 features per split (strong randomness)
  bootstrap: True       ‚Üê Default: each tree sees different data
```

## Commands Reference

```bash
# Submit
python kaggle/kaggle_automate.py "message"

# Check Kaggle API
kaggle competitions submissions -c neurips-open-polymer-prediction-2025 --csv

# View kernel metadata
cat kernel-metadata.json
```

---

**Status:** ü•á **TIED WITH 1ST PLACE!** | **Last Updated:** Nov 13, 2025 | **Best Model:** v85 Random Forest Ensemble (Canonicalization + Pseudo-labeling) | **Score:** 0.07533 (Private) | 0.08139 (Public) | **Improvement:** 45.8% from baseline
