{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75193dfb",
   "metadata": {
    "papermill": {
     "duration": 0.007361,
     "end_time": "2025-10-31T18:09:07.543505",
     "exception": false,
     "start_time": "2025-10-31T18:09:07.536144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Open Polymer Property Prediction - AutoGluon Production\n",
    "\n",
    "**ü§ñ Using Pre-Trained AutoGluon Models**\n",
    "\n",
    "**AutoGluon: WeightedEnsemble_L2 with 34 Features + Full Data Augmentation**\n",
    "\n",
    "This notebook uses pre-trained AutoGluon models from `train_autogluon_production.py` for production inference:\n",
    "\n",
    "**Target properties:** Tg (glass transition temp), FFV (free volume fraction), Tc (crystallization temp), Density, Rg (radius of gyration)\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ **AutoGluon WeightedEnsemble_L2** - Intelligent stacking of 8 base models\n",
    "- ‚úÖ **34 Comprehensive Features** - 10 simple + 11 hand-crafted + 13 RDKit descriptors\n",
    "- ‚úÖ **Full Data Augmentation** - 60K+ samples from original + external + pseudo-labels\n",
    "- ‚úÖ **SMILES Canonicalization** - Standardizes molecular representations\n",
    "- ‚úÖ **Automatic Hyperparameter Tuning** - AutoGluon optimizes for each algorithm\n",
    "- ‚úÖ **Tg Transformation** - (9/5)√óTg + 45 (2nd place discovery)\n",
    "- ‚úÖ **MAE Objective** - AutoGluon aligns with competition metric\n",
    "\n",
    "**Data Augmentation Impact:**\n",
    "- **Original training:** 7,973 samples\n",
    "- **With external Tc/Tg/Density/Rg:** ~17,000 samples\n",
    "- **With 50K Pseudo-Labels:** ~60,000+ training samples\n",
    "- **AutoGluon handles:** Automatic feature selection from 34 features\n",
    "\n",
    "**Why This Works:**\n",
    "- AutoGluon's stacked ensemble reduces individual model bias\n",
    "- 34 features provide rich signal; AutoGluon selects most predictive\n",
    "- 60K+ samples enable robust training with proper regularization\n",
    "- Canonicalization ensures consistent molecular representation\n",
    "- Tg transformation corrects for train/test distribution shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b8268f",
   "metadata": {
    "papermill": {
     "duration": 0.00523,
     "end_time": "2025-10-31T18:09:07.554387",
     "exception": false,
     "start_time": "2025-10-31T18:09:07.549157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8759d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:07.567020Z",
     "iopub.status.busy": "2025-10-31T18:09:07.566697Z",
     "iopub.status.idle": "2025-10-31T18:09:19.431779Z",
     "shell.execute_reply": "2025-10-31T18:09:19.430794Z"
    },
    "papermill": {
     "duration": 11.873921,
     "end_time": "2025-10-31T18:09:19.433694",
     "exception": false,
     "start_time": "2025-10-31T18:09:07.559773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup for AutoGluon production inference\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Force CPU-only mode for AutoGluon (avoids MPS hanging on Apple Silicon)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MPS_ENABLED'] = '0'\n",
    "\n",
    "# Try to import RDKit for SMILES canonicalization and descriptors\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, AllChem\n",
    "    RDKIT_AVAILABLE = True\n",
    "    print(\"‚úì RDKit available for SMILES canonicalization and RDKit descriptors\")\n",
    "except ImportError:\n",
    "    RDKIT_AVAILABLE = False\n",
    "    Chem = None\n",
    "    print(\"‚ö† RDKit not available - will use fallback features\")\n",
    "\n",
    "# Try to import AutoGluon\n",
    "try:\n",
    "    from autogluon.tabular import TabularPredictor\n",
    "    AUTOGLUON_AVAILABLE = True\n",
    "    print(\"‚úì AutoGluon available for production inference\")\n",
    "except ImportError:\n",
    "    AUTOGLUON_AVAILABLE = False\n",
    "    print(\"‚ö† AutoGluon not installed - cannot perform AutoGluon inference\")\n",
    "\n",
    "# ============================================================================\n",
    "# Feature Strategy: 34 Comprehensive Features (AutoGluon Production)\n",
    "# ============================================================================\n",
    "# AutoGluon production uses:\n",
    "# - 10 simple string-based features (fast, reliable)\n",
    "# - 11 chemistry-based features (polymer-specific domain knowledge)\n",
    "# - 13 RDKit molecular descriptors (chemical properties)\n",
    "# Total: 34 features for AutoGluon to automatically select from\n",
    "\n",
    "target_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "# SMILES canonicalization function\n",
    "def make_smile_canonical(smile):\n",
    "    \"\"\"To avoid duplicates, for example: canonical '*C=C(*)C' == '*C(=C*)C'\"\"\"\n",
    "    if not RDKIT_AVAILABLE or Chem is None:\n",
    "        return smile  # Return as-is if RDKit not available\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is None:\n",
    "            return np.nan\n",
    "        canon_smile = Chem.MolToSmiles(mol, canonical=True)\n",
    "        return canon_smile\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def extract_comprehensive_features(smiles_str):\n",
    "    \"\"\"Extract all 34 features: 10 simple + 11 chemistry + 13 RDKit descriptors\"\"\"\n",
    "    try:\n",
    "        # 10 simple string-based features\n",
    "        basic = {\n",
    "            'smiles_length': len(smiles_str),\n",
    "            'carbon_count': smiles_str.count('C'),\n",
    "            'nitrogen_count': smiles_str.count('N'),\n",
    "            'oxygen_count': smiles_str.count('O'),\n",
    "            'sulfur_count': smiles_str.count('S'),\n",
    "            'fluorine_count': smiles_str.count('F'),\n",
    "            'ring_count': smiles_str.count('c') + smiles_str.count('C1'),\n",
    "            'double_bond_count': smiles_str.count('='),\n",
    "            'triple_bond_count': smiles_str.count('#'),\n",
    "            'branch_count': smiles_str.count('('),\n",
    "        }\n",
    "        \n",
    "        # 11 chemistry-based features\n",
    "        num_side_chains = smiles_str.count('(')\n",
    "        backbone_carbons = smiles_str.count('C') - smiles_str.count('C(')\n",
    "        aromatic_count = smiles_str.count('c')\n",
    "        h_bond_donors = smiles_str.count('O') + smiles_str.count('N')\n",
    "        h_bond_acceptors = smiles_str.count('O') + smiles_str.count('N')\n",
    "        num_rings = smiles_str.count('1') + smiles_str.count('2')\n",
    "        single_bonds = len(smiles_str) - smiles_str.count('=') - smiles_str.count('#') - aromatic_count\n",
    "        halogen_count = smiles_str.count('F') + smiles_str.count('Cl') + smiles_str.count('Br')\n",
    "        heteroatom_count = smiles_str.count('N') + smiles_str.count('O') + smiles_str.count('S')\n",
    "        mw_estimate = (smiles_str.count('C') * 12 + smiles_str.count('O') * 16 + \n",
    "                      smiles_str.count('N') * 14 + smiles_str.count('S') * 32 + smiles_str.count('F') * 19)\n",
    "        branching_ratio = num_side_chains / max(backbone_carbons, 1)\n",
    "        \n",
    "        chemistry = {\n",
    "            'num_side_chains': num_side_chains,\n",
    "            'backbone_carbons': backbone_carbons,\n",
    "            'branching_ratio': branching_ratio,\n",
    "            'aromatic_count': aromatic_count,\n",
    "            'h_bond_donors': h_bond_donors,\n",
    "            'h_bond_acceptors': h_bond_acceptors,\n",
    "            'num_rings': num_rings,\n",
    "            'single_bonds': single_bonds,\n",
    "            'halogen_count': halogen_count,\n",
    "            'heteroatom_count': heteroatom_count,\n",
    "            'mw_estimate': mw_estimate,\n",
    "        }\n",
    "        \n",
    "        # 13 RDKit descriptors (if available)\n",
    "        rdkit_desc = {}\n",
    "        if RDKIT_AVAILABLE and Chem is not None:\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles_str)\n",
    "                if mol is not None:\n",
    "                    rdkit_desc = {\n",
    "                        'MolWt': Descriptors.MolWt(mol),\n",
    "                        'LogP': Descriptors.MolLogP(mol),\n",
    "                        'NumHDonors': Descriptors.NumHDonors(mol),\n",
    "                        'NumHAcceptors': Descriptors.NumHAcceptors(mol),\n",
    "                        'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
    "                        'NumAromaticRings': Descriptors.NumAromaticRings(mol),\n",
    "                        'TPSA': Descriptors.TPSA(mol),\n",
    "                        'NumSaturatedRings': Descriptors.NumSaturatedRings(mol),\n",
    "                        'NumAliphaticRings': Descriptors.NumAliphaticRings(mol),\n",
    "                        'RingCount': Descriptors.RingCount(mol),\n",
    "                        'FractionCsp3': Descriptors.FractionCsp3(mol),\n",
    "                        'NumHeteroatoms': Descriptors.NumHeteroatoms(mol),\n",
    "                        'BertzCT': Descriptors.BertzCT(mol),\n",
    "                    }\n",
    "                    # Replace any NaN/inf with 0\n",
    "                    for k, v in rdkit_desc.items():\n",
    "                        if pd.isna(v) or np.isinf(v):\n",
    "                            rdkit_desc[k] = 0.0\n",
    "            except:\n",
    "                # Fill with zeros if RDKit fails\n",
    "                rdkit_desc = {k: 0.0 for k in ['MolWt', 'LogP', 'NumHDonors', 'NumHAcceptors', \n",
    "                                               'NumRotatableBonds', 'NumAromaticRings', 'TPSA',\n",
    "                                               'NumSaturatedRings', 'NumAliphaticRings', 'RingCount',\n",
    "                                               'FractionCsp3', 'NumHeteroatoms', 'BertzCT']}\n",
    "        else:\n",
    "            # Fallback: fill with zeros\n",
    "            rdkit_desc = {k: 0.0 for k in ['MolWt', 'LogP', 'NumHDonors', 'NumHAcceptors', \n",
    "                                           'NumRotatableBonds', 'NumAromaticRings', 'TPSA',\n",
    "                                           'NumSaturatedRings', 'NumAliphaticRings', 'RingCount',\n",
    "                                           'FractionCsp3', 'NumHeteroatoms', 'BertzCT']}\n",
    "        \n",
    "        # Combine all features\n",
    "        return {**basic, **chemistry, **rdkit_desc}\n",
    "    except:\n",
    "        # Ultimate fallback: all zeros\n",
    "        return {\n",
    "            'smiles_length': 0, 'carbon_count': 0, 'nitrogen_count': 0, 'oxygen_count': 0,\n",
    "            'sulfur_count': 0, 'fluorine_count': 0, 'ring_count': 0, 'double_bond_count': 0,\n",
    "            'triple_bond_count': 0, 'branch_count': 0, 'num_side_chains': 0, 'backbone_carbons': 0,\n",
    "            'branching_ratio': 0, 'aromatic_count': 0, 'h_bond_donors': 0, 'h_bond_acceptors': 0,\n",
    "            'num_rings': 0, 'single_bonds': 0, 'halogen_count': 0, 'heteroatom_count': 0,\n",
    "            'mw_estimate': 0, 'MolWt': 0, 'LogP': 0, 'NumHDonors': 0, 'NumHAcceptors': 0,\n",
    "            'NumRotatableBonds': 0, 'NumAromaticRings': 0, 'TPSA': 0, 'NumSaturatedRings': 0,\n",
    "            'NumAliphaticRings': 0, 'RingCount': 0, 'FractionCsp3': 0, 'NumHeteroatoms': 0, 'BertzCT': 0\n",
    "        }\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"AUTOGLUON PRODUCTION SETUP - 34 Comprehensive Features\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Simple Features (10): smiles_length, C/N/O/S/F counts, rings, bonds, branches\")\n",
    "print(\"Chemistry Features (11): side_chains, backbone, branching, aromatic, H-bonding, etc.\")\n",
    "print(\"RDKit Descriptors (13): MolWt, LogP, TPSA, rotatable bonds, aromaticity, etc.\")\n",
    "print(\"Total: 34 features for AutoGluon automatic selection\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"‚úì Setup complete! Ready for AutoGluon inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7c643",
   "metadata": {
    "papermill": {
     "duration": 0.005882,
     "end_time": "2025-10-31T18:09:19.445573",
     "exception": false,
     "start_time": "2025-10-31T18:09:19.439691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0c1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:19.458887Z",
     "iopub.status.busy": "2025-10-31T18:09:19.457881Z",
     "iopub.status.idle": "2025-10-31T18:09:19.525996Z",
     "shell.execute_reply": "2025-10-31T18:09:19.524794Z"
    },
    "papermill": {
     "duration": 0.077637,
     "end_time": "2025-10-31T18:09:19.528760",
     "exception": false,
     "start_time": "2025-10-31T18:09:19.451123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data with error handling\n",
    "try:\n",
    "    train_df = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')\n",
    "    test_df = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n",
    "    sample_submission = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv')\n",
    "    print(\"Data loaded from Kaggle input\")\n",
    "except:\n",
    "    try:\n",
    "        # Fallback for local testing\n",
    "        train_df = pd.read_csv('data/raw/train.csv')\n",
    "        test_df = pd.read_csv('data/raw/test.csv')\n",
    "        sample_submission = pd.read_csv('data/raw/sample_submission.csv')\n",
    "        print(\"Data loaded from local files\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "\n",
    "print(\"\\nTarget availability:\")\n",
    "for col in target_cols:\n",
    "    n_avail = train_df[col].notna().sum()\n",
    "    print(f\"{col}: {n_avail} samples ({n_avail/len(train_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c442c16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:19.543826Z",
     "iopub.status.busy": "2025-10-31T18:09:19.543464Z",
     "iopub.status.idle": "2025-10-31T18:09:19.575108Z",
     "shell.execute_reply": "2025-10-31T18:09:19.574077Z"
    },
    "papermill": {
     "duration": 0.040022,
     "end_time": "2025-10-31T18:09:19.576609",
     "exception": false,
     "start_time": "2025-10-31T18:09:19.536587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Canonicalize SMILES to avoid duplicates and standardize representations\n",
    "print(\"=\" * 70)\n",
    "print(\"CANONICALIZING SMILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if RDKIT_AVAILABLE:\n",
    "    print(\"Applying SMILES canonicalization...\")\n",
    "    \n",
    "    # Store original counts\n",
    "    orig_train_count = len(train_df)\n",
    "    orig_test_count = len(test_df)\n",
    "    \n",
    "    # Apply canonicalization\n",
    "    train_df['SMILES_canonical'] = train_df['SMILES'].apply(make_smile_canonical)\n",
    "    test_df['SMILES_canonical'] = test_df['SMILES'].apply(make_smile_canonical)\n",
    "    \n",
    "    # Count successes\n",
    "    train_success = train_df['SMILES_canonical'].notna().sum()\n",
    "    test_success = test_df['SMILES_canonical'].notna().sum()\n",
    "    \n",
    "    print(f\"Train: {train_success}/{orig_train_count} successfully canonicalized ({train_success/orig_train_count*100:.1f}%)\")\n",
    "    print(f\"Test: {test_success}/{orig_test_count} successfully canonicalized ({test_success/orig_test_count*100:.1f}%)\")\n",
    "    \n",
    "    # For failed canonicalizations, keep original SMILES\n",
    "    train_df['SMILES_canonical'] = train_df['SMILES_canonical'].fillna(train_df['SMILES'])\n",
    "    test_df['SMILES_canonical'] = test_df['SMILES_canonical'].fillna(test_df['SMILES'])\n",
    "    \n",
    "    # Replace SMILES with canonical versions\n",
    "    train_df['SMILES'] = train_df['SMILES_canonical']\n",
    "    test_df['SMILES'] = test_df['SMILES_canonical']\n",
    "    \n",
    "    # Drop temporary column\n",
    "    train_df = train_df.drop('SMILES_canonical', axis=1)\n",
    "    test_df = test_df.drop('SMILES_canonical', axis=1)\n",
    "    \n",
    "    print(\"‚úì SMILES canonicalization complete!\")\n",
    "    \n",
    "    # Show example\n",
    "    print(\"\\nExample canonical SMILES:\")\n",
    "    print(train_df['SMILES'].head(3).tolist())\n",
    "else:\n",
    "    print(\"‚ö† RDKit not available - skipping canonicalization\")\n",
    "    print(\"Using original SMILES as-is\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d76b4ac",
   "metadata": {
    "papermill": {
     "duration": 0.006179,
     "end_time": "2025-10-31T18:09:19.588756",
     "exception": false,
     "start_time": "2025-10-31T18:09:19.582577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.5 Load and Incorporate External Tc Dataset\n",
    "\n",
    "**Strategy:** The competition training data has only 737 samples for Tc (crystallization temperature). We'll augment this with the external Tc dataset to improve Tc predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c02aa90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:19.603732Z",
     "iopub.status.busy": "2025-10-31T18:09:19.603411Z",
     "iopub.status.idle": "2025-10-31T18:09:19.660136Z",
     "shell.execute_reply": "2025-10-31T18:09:19.658869Z"
    },
    "papermill": {
     "duration": 0.065882,
     "end_time": "2025-10-31T18:09:19.661609",
     "exception": false,
     "start_time": "2025-10-31T18:09:19.595727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load external Tc dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING EXTERNAL Tc DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Load the external Tc data - try multiple possible paths\n",
    "    tc_path = None\n",
    "    possible_paths = [\n",
    "        '/kaggle/input/tc-smiles/Tc_SMILES.csv',\n",
    "        '/kaggle/input/tc-smiles/TC_SMILES.csv',\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            tc_path = path\n",
    "            break\n",
    "    \n",
    "    if not tc_path:\n",
    "        # List available files in tc-smiles directory\n",
    "        import os\n",
    "        tc_dir = '/kaggle/input/tc-smiles'\n",
    "        if os.path.exists(tc_dir):\n",
    "            files = os.listdir(tc_dir)\n",
    "            print(f\"Available files in {tc_dir}: {files}\")\n",
    "            for f in files:\n",
    "                if f.endswith('.csv'):\n",
    "                    tc_path = os.path.join(tc_dir, f)\n",
    "                    break\n",
    "    \n",
    "    if not tc_path:\n",
    "        raise FileNotFoundError(\"No Tc CSV file found\")\n",
    "    \n",
    "    tc_external = pd.read_csv(tc_path)\n",
    "    print(f\"Loaded from: {tc_path}\")\n",
    "    print(f\"‚úì Loaded external Tc dataset: {len(tc_external)} samples\")\n",
    "    print(f\"Columns: {list(tc_external.columns)}\")\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(tc_external.head())\n",
    "    \n",
    "    # Canonicalize external SMILES\n",
    "    if RDKIT_AVAILABLE:\n",
    "        print(\"\\nCanonicalizing external SMILES...\")\n",
    "        tc_external['SMILES_canonical'] = tc_external['SMILES'].apply(make_smile_canonical)\n",
    "        tc_success = tc_external['SMILES_canonical'].notna().sum()\n",
    "        print(f\"External Tc: {tc_success}/{len(tc_external)} successfully canonicalized ({tc_success/len(tc_external)*100:.1f}%)\")\n",
    "        \n",
    "        # For failed canonicalizations, keep original\n",
    "        tc_external['SMILES_canonical'] = tc_external['SMILES_canonical'].fillna(tc_external['SMILES'])\n",
    "        tc_external['SMILES'] = tc_external['SMILES_canonical']\n",
    "        tc_external = tc_external.drop('SMILES_canonical', axis=1)\n",
    "    \n",
    "    # Rename TC_mean to Tc to match training data\n",
    "    tc_external = tc_external.rename(columns={'TC_mean': 'Tc'})\n",
    "    \n",
    "    # Check for overlap with training data\n",
    "    train_smiles = set(train_df['SMILES'])\n",
    "    external_smiles = set(tc_external['SMILES'])\n",
    "    overlap = train_smiles & external_smiles\n",
    "    print(f\"\\nüìä Dataset overlap analysis:\")\n",
    "    print(f\"Training SMILES: {len(train_smiles)}\")\n",
    "    print(f\"External SMILES: {len(external_smiles)}\")\n",
    "    print(f\"Overlapping SMILES: {len(overlap)}\")\n",
    "    \n",
    "    # Get original Tc count in training\n",
    "    orig_tc_count = train_df['Tc'].notna().sum()\n",
    "    print(f\"\\nOriginal training Tc samples: {orig_tc_count}\")\n",
    "    \n",
    "    # Merge strategy: Add external data for SMILES NOT in training set\n",
    "    # For overlapping SMILES, we keep training data (more reliable)\n",
    "    tc_new = tc_external[~tc_external['SMILES'].isin(train_smiles)].copy()\n",
    "    print(f\"New Tc samples to add: {len(tc_new)}\")\n",
    "    \n",
    "    if len(tc_new) > 0:\n",
    "        # Create rows with only SMILES and Tc filled\n",
    "        tc_new_rows = []\n",
    "        for _, row in tc_new.iterrows():\n",
    "            new_row = {\n",
    "                'SMILES': row['SMILES'],\n",
    "                'Tg': np.nan,\n",
    "                'FFV': np.nan,\n",
    "                'Tc': row['Tc'],\n",
    "                'Density': np.nan,\n",
    "                'Rg': np.nan\n",
    "            }\n",
    "            tc_new_rows.append(new_row)\n",
    "        \n",
    "        tc_new_df = pd.DataFrame(tc_new_rows)\n",
    "        \n",
    "        # Append to training data\n",
    "        train_df_original = train_df.copy()\n",
    "        train_df = pd.concat([train_df, tc_new_df], ignore_index=True)\n",
    "        \n",
    "        new_tc_count = train_df['Tc'].notna().sum()\n",
    "        print(f\"\\n‚úÖ AUGMENTATION COMPLETE!\")\n",
    "        print(f\"Training set size: {len(train_df_original)} ‚Üí {len(train_df)} (+{len(tc_new)})\")\n",
    "        print(f\"Tc samples: {orig_tc_count} ‚Üí {new_tc_count} (+{new_tc_count - orig_tc_count})\")\n",
    "        print(f\"Tc improvement: {((new_tc_count - orig_tc_count) / orig_tc_count * 100):.1f}% increase\")\n",
    "        \n",
    "        print(f\"\\nüìà Final training data statistics:\")\n",
    "        for col in target_cols:\n",
    "            n_avail = train_df[col].notna().sum()\n",
    "            print(f\"  {col}: {n_avail} samples ({n_avail/len(train_df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö† All external SMILES already in training set - no augmentation needed\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö† External Tc dataset not found - skipping augmentation\")\n",
    "    print(\"Continuing with original training data only\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error loading external Tc data: {e}\")\n",
    "    print(\"Continuing with original training data only\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7b0b0",
   "metadata": {
    "papermill": {
     "duration": 0.005529,
     "end_time": "2025-10-31T18:09:19.673062",
     "exception": false,
     "start_time": "2025-10-31T18:09:19.667533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.6 Load and Incorporate External Tg Dataset\n",
    "\n",
    "**Strategy:** The competition training data has only 511 samples for Tg (glass transition temperature) - the LEAST represented property! We'll augment this with 7,000+ external Tg samples for massive improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59f97db2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:19.686337Z",
     "iopub.status.busy": "2025-10-31T18:09:19.685912Z",
     "iopub.status.idle": "2025-10-31T18:09:19.852119Z",
     "shell.execute_reply": "2025-10-31T18:09:19.850822Z"
    },
    "papermill": {
     "duration": 0.174982,
     "end_time": "2025-10-31T18:09:19.853803",
     "exception": false,
     "start_time": "2025-10-31T18:09:19.678821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load external Tg dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING EXTERNAL Tg DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Load the external Tg data\n",
    "    tg_external = pd.read_csv('/kaggle/input/tg-of-polymer-dataset/Tg_SMILES_class_pid_polyinfo_median.csv')\n",
    "    print(f\"‚úì Loaded external Tg dataset: {len(tg_external)} samples\")\n",
    "    print(f\"Columns: {list(tg_external.columns)}\")\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(tg_external.head())\n",
    "    \n",
    "    # Canonicalize external SMILES\n",
    "    if RDKIT_AVAILABLE:\n",
    "        print(\"\\nCanonicalizing external SMILES...\")\n",
    "        tg_external['SMILES_canonical'] = tg_external['SMILES'].apply(make_smile_canonical)\n",
    "        tg_success = tg_external['SMILES_canonical'].notna().sum()\n",
    "        print(f\"External Tg: {tg_success}/{len(tg_external)} successfully canonicalized ({tg_success/len(tg_external)*100:.1f}%)\")\n",
    "        \n",
    "        # For failed canonicalizations, keep original\n",
    "        tg_external['SMILES_canonical'] = tg_external['SMILES_canonical'].fillna(tg_external['SMILES'])\n",
    "        tg_external['SMILES'] = tg_external['SMILES_canonical']\n",
    "        tg_external = tg_external.drop('SMILES_canonical', axis=1)\n",
    "    \n",
    "    # Check for overlap with training data\n",
    "    train_smiles = set(train_df['SMILES'])\n",
    "    external_smiles = set(tg_external['SMILES'])\n",
    "    overlap = train_smiles & external_smiles\n",
    "    print(f\"\\nüìä Dataset overlap analysis:\")\n",
    "    print(f\"Training SMILES: {len(train_smiles)}\")\n",
    "    print(f\"External SMILES: {len(external_smiles)}\")\n",
    "    print(f\"Overlapping SMILES: {len(overlap)}\")\n",
    "    \n",
    "    # Get original Tg count in training\n",
    "    orig_tg_count = train_df['Tg'].notna().sum()\n",
    "    print(f\"\\nOriginal training Tg samples: {orig_tg_count}\")\n",
    "    \n",
    "    # Merge strategy: Add external data for SMILES NOT in training set\n",
    "    # For overlapping SMILES, we keep training data (more reliable)\n",
    "    tg_new = tg_external[~tg_external['SMILES'].isin(train_smiles)].copy()\n",
    "    print(f\"New Tg samples to add: {len(tg_new)}\")\n",
    "    \n",
    "    if len(tg_new) > 0:\n",
    "        # Create rows with only SMILES and Tg filled\n",
    "        tg_new_rows = []\n",
    "        for _, row in tg_new.iterrows():\n",
    "            new_row = {\n",
    "                'SMILES': row['SMILES'],\n",
    "                'Tg': row['Tg'],\n",
    "                'FFV': np.nan,\n",
    "                'Tc': np.nan,\n",
    "                'Density': np.nan,\n",
    "                'Rg': np.nan\n",
    "            }\n",
    "            tg_new_rows.append(new_row)\n",
    "        \n",
    "        tg_new_df = pd.DataFrame(tg_new_rows)\n",
    "        \n",
    "        # Append to training data\n",
    "        train_df_before_tg = train_df.copy()\n",
    "        train_df = pd.concat([train_df, tg_new_df], ignore_index=True)\n",
    "        \n",
    "        new_tg_count = train_df['Tg'].notna().sum()\n",
    "        print(f\"\\n‚úÖ Tg AUGMENTATION COMPLETE!\")\n",
    "        print(f\"Training set size: {len(train_df_before_tg)} ‚Üí {len(train_df)} (+{len(tg_new)})\")\n",
    "        print(f\"Tg samples: {orig_tg_count} ‚Üí {new_tg_count} (+{new_tg_count - orig_tg_count})\")\n",
    "        print(f\"Tg improvement: {((new_tg_count - orig_tg_count) / orig_tg_count * 100):.1f}% increase\")\n",
    "        \n",
    "        print(f\"\\nüìà Final training data statistics:\")\n",
    "        for col in target_cols:\n",
    "            n_avail = train_df[col].notna().sum()\n",
    "            print(f\"  {col}: {n_avail} samples ({n_avail/len(train_df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö† All external SMILES already in training set - no augmentation needed\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö† External Tg dataset not found - skipping augmentation\")\n",
    "    print(\"Continuing with original training data only\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error loading external Tg data: {e}\")\n",
    "    print(\"Continuing with original training data only\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c0f65c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:19.867641Z",
     "iopub.status.busy": "2025-10-31T18:09:19.867333Z",
     "iopub.status.idle": "2025-10-31T18:09:29.645073Z",
     "shell.execute_reply": "2025-10-31T18:09:29.644004Z"
    },
    "papermill": {
     "duration": 9.786574,
     "end_time": "2025-10-31T18:09:29.646587",
     "exception": false,
     "start_time": "2025-10-31T18:09:19.860013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and Integrate External Datasets\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING EXTERNAL DATASETS FOR AUGMENTATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load PI1070 dataset (Density + Rg)\n",
    "print(\"\\n[1] Loading PI1070.csv (Density + Rg)...\")\n",
    "try:\n",
    "    pi1070_df = pd.read_csv('/kaggle/input/more-data/PI1070.csv')\n",
    "    print(f\"‚úì Loaded {len(pi1070_df)} samples\")\n",
    "    print(f\"  Columns: {list(pi1070_df.columns)[:5]}... (truncated)\")\n",
    "    \n",
    "    # Extract SMILES, Density, Rg\n",
    "    pi1070_subset = pi1070_df[['smiles', 'density', 'Rg']].copy()\n",
    "    pi1070_subset = pi1070_subset.rename(columns={'smiles': 'SMILES'})\n",
    "    \n",
    "    # Check for overlaps\n",
    "    pi1070_smiles = set(pi1070_subset['SMILES'].dropna())\n",
    "    train_smiles_set = set(train_df['SMILES'].dropna())\n",
    "    overlap_pi1070 = len(pi1070_smiles & train_smiles_set)\n",
    "    pi1070_new = pi1070_subset[~pi1070_subset['SMILES'].isin(train_smiles_set)].copy()\n",
    "    \n",
    "    print(f\"  New non-overlapping samples: {len(pi1070_new)}\")\n",
    "    print(f\"  Density values available: {pi1070_new['density'].notna().sum()}\")\n",
    "    print(f\"  Rg values available: {pi1070_new['Rg'].notna().sum()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Failed to load PI1070: {e}\")\n",
    "    pi1070_new = None\n",
    "\n",
    "# Load LAMALAB Tg dataset\n",
    "print(\"\\n[2] Loading LAMALAB_CURATED_Tg_structured_polymerclass.csv...\")\n",
    "try:\n",
    "    lamalab_df = pd.read_csv('/kaggle/input/more-data/LAMALAB_CURATED_Tg_structured_polymerclass.csv')\n",
    "    print(f\"‚úì Loaded {len(lamalab_df)} samples\")\n",
    "    \n",
    "    # Extract SMILES and Tg (convert from Kelvin to Celsius)\n",
    "    lamalab_subset = lamalab_df[['PSMILES', 'labels.Exp_Tg(K)']].copy()\n",
    "    lamalab_subset = lamalab_subset.rename(columns={'PSMILES': 'SMILES', 'labels.Exp_Tg(K)': 'Tg'})\n",
    "    \n",
    "    # Convert Tg from Kelvin to Celsius\n",
    "    lamalab_subset['Tg'] = lamalab_subset['Tg'] - 273.15\n",
    "    \n",
    "    # Check for overlaps\n",
    "    lamalab_smiles = set(lamalab_subset['SMILES'].dropna())\n",
    "    overlap_lamalab = len(lamalab_smiles & train_smiles_set)\n",
    "    lamalab_new = lamalab_subset[~lamalab_subset['SMILES'].isin(train_smiles_set)].copy()\n",
    "    \n",
    "    print(f\"  New non-overlapping samples: {len(lamalab_new)}\")\n",
    "    print(f\"  Tg values available: {lamalab_new['Tg'].notna().sum()}\")\n",
    "    print(f\"  Tg range (¬∞C): [{lamalab_new['Tg'].min():.1f}, {lamalab_new['Tg'].max():.1f}]\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Failed to load LAMALAB Tg: {e}\")\n",
    "    lamalab_new = None\n",
    "\n",
    "# Augment training data\n",
    "print(\"\\n[3] Augmenting training data...\")\n",
    "train_df_before = len(train_df)\n",
    "\n",
    "# Add PI1070 data (Density + Rg)\n",
    "if pi1070_new is not None and len(pi1070_new) > 0:\n",
    "    for idx, row in pi1070_new.iterrows():\n",
    "        if pd.notna(row['density']) or pd.notna(row['Rg']):\n",
    "            train_df = pd.concat([train_df, pd.DataFrame([{\n",
    "                'SMILES': row['SMILES'],\n",
    "                'Tg': np.nan,\n",
    "                'FFV': np.nan,\n",
    "                'Tc': np.nan,\n",
    "                'Density': row['density'] if pd.notna(row['density']) else np.nan,\n",
    "                'Rg': row['Rg'] if pd.notna(row['Rg']) else np.nan\n",
    "            }])], ignore_index=True)\n",
    "    print(f\"‚úì Added {len(pi1070_new)} PI1070 samples\")\n",
    "\n",
    "# Add LAMALAB Tg data\n",
    "if lamalab_new is not None and len(lamalab_new) > 0:\n",
    "    lamalab_new_valid = lamalab_new[lamalab_new['Tg'].notna()].copy()\n",
    "    if len(lamalab_new_valid) > 0:\n",
    "        for idx, row in lamalab_new_valid.iterrows():\n",
    "            train_df = pd.concat([train_df, pd.DataFrame([{\n",
    "                'SMILES': row['SMILES'],\n",
    "                'Tg': row['Tg'],\n",
    "                'FFV': np.nan,\n",
    "                'Tc': np.nan,\n",
    "                'Density': np.nan,\n",
    "                'Rg': np.nan\n",
    "            }])], ignore_index=True)\n",
    "        print(f\"‚úì Added {len(lamalab_new_valid)} LAMALAB Tg samples\")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nüìä Training data augmented:\")\n",
    "print(f\"  Before: {train_df_before} samples\")\n",
    "print(f\"  After: {len(train_df)} samples\")\n",
    "print(f\"  Net increase: +{len(train_df) - train_df_before} samples ({100*(len(train_df)-train_df_before)/train_df_before:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìà Updated target availability:\")\n",
    "for col in target_cols:\n",
    "    n_avail = train_df[col].notna().sum()\n",
    "    print(f\"    {col}: {n_avail} samples ({n_avail/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pseudo-label-cell",
   "metadata": {},
   "source": [
    "## 2.7 Load and Incorporate Pseudo-Labeled Dataset\n",
    "\n",
    "**Strategy:** Add 50,000 pseudo-labeled samples generated from ensemble of BERT, AutoGluon, and Uni-Mol.\n",
    "This provides massive additional training data to improve all property predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pseudo-label-code-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pseudo-labeled dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING PSEUDO-LABELED DATASET (Ensemble: BERT + AutoGluon + Uni-Mol)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Try loading from Kaggle input first\n",
    "    pseudo_label_path = None\n",
    "    \n",
    "    # First, check what files are in the pi1m-pseudolabels directory\n",
    "    pi1m_dir = '/kaggle/input/pi1m-pseudolabels'\n",
    "    if os.path.exists(pi1m_dir):\n",
    "        print(f\"Files in {pi1m_dir}:\")\n",
    "        try:\n",
    "            files = os.listdir(pi1m_dir)\n",
    "            for f in files[:10]:  # Show first 10 files\n",
    "                print(f\"  - {f}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try various possible paths\n",
    "    possible_paths = [\n",
    "        '/kaggle/input/pi1m-pseudolabels/PI1M_50000_v2.1.csv',\n",
    "        '/kaggle/input/pi1m-pseudolabels/pi1m_50000_v2.1.csv',  # lowercase\n",
    "        '/kaggle/input/pi1m-pseudolabels/data.csv',  # might be renamed\n",
    "        '/kaggle/input/pseudo-labels/PI1M_50000_v2.1.csv',\n",
    "        'data/PI1M_50000_v2.1.csv',\n",
    "    ]\n",
    "    \n",
    "    # Also try to find any CSV file in pi1m directory\n",
    "    if os.path.exists(pi1m_dir):\n",
    "        try:\n",
    "            for f in os.listdir(pi1m_dir):\n",
    "                if f.endswith('.csv'):\n",
    "                    possible_paths.insert(0, os.path.join(pi1m_dir, f))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            if os.path.exists(path):\n",
    "                pseudo_label_path = path\n",
    "                print(f\"‚úì Found pseudo-label file at: {path}\")\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if pseudo_label_path:\n",
    "        pseudo_df = pd.read_csv(pseudo_label_path)\n",
    "        print(f\"‚úì Loaded pseudo-labeled dataset from: {pseudo_label_path}\")\n",
    "        print(f\"  Samples: {len(pseudo_df)}\")\n",
    "        print(f\"  Columns: {list(pseudo_df.columns)}\")\n",
    "        print(f\"  Source: Ensemble of BERT, AutoGluon, Uni-Mol\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\n  Sample data:\")\n",
    "        print(pseudo_df.head(2))\n",
    "        \n",
    "        # Check for overlap with training data\n",
    "        train_smiles_set = set(train_df['SMILES'].dropna())\n",
    "        pseudo_smiles = set(pseudo_df['SMILES'].dropna())\n",
    "        overlap = len(train_smiles_set & pseudo_smiles)\n",
    "        \n",
    "        print(f\"\\n  üìä Dataset overlap analysis:\")\n",
    "        print(f\"    Training SMILES: {len(train_smiles_set)}\")\n",
    "        print(f\"    Pseudo-label SMILES: {len(pseudo_smiles)}\")\n",
    "        print(f\"    Overlapping SMILES: {overlap}\")\n",
    "        \n",
    "        # Get new non-overlapping samples\n",
    "        pseudo_new = pseudo_df[~pseudo_df['SMILES'].isin(train_smiles_set)].copy()\n",
    "        print(f\"    New samples to add: {len(pseudo_new)}\")\n",
    "        \n",
    "        if len(pseudo_new) > 0:\n",
    "            # Store original sizes\n",
    "            orig_train_size = len(train_df)\n",
    "            orig_counts = {col: train_df[col].notna().sum() for col in target_cols}\n",
    "            \n",
    "            # Append pseudo-labeled data\n",
    "            train_df = pd.concat([train_df, pseudo_new], ignore_index=True)\n",
    "            \n",
    "            print(f\"\\n  ‚úÖ PSEUDO-LABEL AUGMENTATION COMPLETE!\")\n",
    "            print(f\"    Training set size: {orig_train_size} ‚Üí {len(train_df)} (+{len(pseudo_new)})\")\n",
    "            print(f\"    Size increase: +{len(pseudo_new)/orig_train_size*100:.1f}%\")\n",
    "            \n",
    "            print(f\"\\n  üìà Updated target availability:\")\n",
    "            for col in target_cols:\n",
    "                new_count = train_df[col].notna().sum()\n",
    "                increase = new_count - orig_counts[col]\n",
    "                print(f\"    {col}: {orig_counts[col]} ‚Üí {new_count} (+{increase}, +{increase/orig_counts[col]*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"\\n  ‚ö† All pseudo-label SMILES already in training set - no augmentation needed\")\n",
    "    else:\n",
    "        print(\"‚ö† Pseudo-labeled dataset not found in any expected location\")\n",
    "        print(\"Continuing with original training data only\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error loading pseudo-labeled data: {e}\")\n",
    "    print(\"Continuing with original training data only\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77dc5f6",
   "metadata": {
    "papermill": {
     "duration": 0.006082,
     "end_time": "2025-10-31T18:09:29.659367",
     "exception": false,
     "start_time": "2025-10-31T18:09:29.653285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Robust Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5e74f",
   "metadata": {
    "papermill": {
     "duration": 0.006157,
     "end_time": "2025-10-31T18:09:29.671714",
     "exception": false,
     "start_time": "2025-10-31T18:09:29.665557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ‚ö° Critical Optimization: Metric Alignment\n",
    "\n",
    "**Problem:** Most ML models optimize for **squared error (MSE)** by default, but the competition uses **weighted Mean Absolute Error (wMAE)**.\n",
    "\n",
    "**Competition Metric (wMAE):**\n",
    "```\n",
    "wMAE = (1/|X|) * Œ£ Œ£ w_i * |y_pred_i - y_true_i|\n",
    "\n",
    "Where:\n",
    "  w_i = (1/range_i) * (K * sqrt(1/n_i)) / Œ£ sqrt(1/n_j)\n",
    "  \n",
    "  - range_i = max - min for property i\n",
    "  - n_i = number of available samples for property i\n",
    "  - K = number of properties (5)\n",
    "```\n",
    "\n",
    "**Key differences:**\n",
    "- **MAE vs MSE:** MAE is less sensitive to outliers\n",
    "- **Weighting:** Properties with fewer samples and smaller ranges get higher weights\n",
    "- **Sparse labels:** Each property has different coverage\n",
    "\n",
    "**Solution:** Use `objective='reg:absoluteerror'` in XGBoost to align with competition metric!\n",
    "\n",
    "This alignment could improve scores by **5-15%** compared to default squared error optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365175fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:29.686900Z",
     "iopub.status.busy": "2025-10-31T18:09:29.686580Z",
     "iopub.status.idle": "2025-10-31T18:09:29.714747Z",
     "shell.execute_reply": "2025-10-31T18:09:29.713697Z"
    },
    "papermill": {
     "duration": 0.037763,
     "end_time": "2025-10-31T18:09:29.716248",
     "exception": false,
     "start_time": "2025-10-31T18:09:29.678485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: Feature extraction is now done directly via extract_comprehensive_features()\n",
    "# No need for RobustMolecularProcessor class anymore - we use AutoGluon for intelligent feature selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea745c4",
   "metadata": {
    "papermill": {
     "duration": 0.006385,
     "end_time": "2025-10-31T18:09:29.729259",
     "exception": false,
     "start_time": "2025-10-31T18:09:29.722874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Robust Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2bc761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:29.743229Z",
     "iopub.status.busy": "2025-10-31T18:09:29.742885Z",
     "iopub.status.idle": "2025-10-31T18:09:29.748578Z",
     "shell.execute_reply": "2025-10-31T18:09:29.747216Z"
    },
    "papermill": {
     "duration": 0.014513,
     "end_time": "2025-10-31T18:09:29.750133",
     "exception": false,
     "start_time": "2025-10-31T18:09:29.735620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fd956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:29.764669Z",
     "iopub.status.busy": "2025-10-31T18:09:29.764385Z",
     "iopub.status.idle": "2025-10-31T18:09:29.777731Z",
     "shell.execute_reply": "2025-10-31T18:09:29.776734Z"
    },
    "papermill": {
     "duration": 0.02261,
     "end_time": "2025-10-31T18:09:29.779281",
     "exception": false,
     "start_time": "2025-10-31T18:09:29.756671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RandomForestModel class removed - we now use pre-trained AutoGluon models!\n",
    "# AutoGluon provides WeightedEnsemble_L2 which is superior to manual Random Forest ensembles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85493de",
   "metadata": {
    "papermill": {
     "duration": 0.006213,
     "end_time": "2025-10-31T18:09:29.792562",
     "exception": false,
     "start_time": "2025-10-31T18:09:29.786349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Load Pre-Trained AutoGluon Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94712d91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:29.806674Z",
     "iopub.status.busy": "2025-10-31T18:09:29.806253Z",
     "iopub.status.idle": "2025-10-31T18:09:29.949271Z",
     "shell.execute_reply": "2025-10-31T18:09:29.948074Z"
    },
    "papermill": {
     "duration": 0.151897,
     "end_time": "2025-10-31T18:09:29.950853",
     "exception": false,
     "start_time": "2025-10-31T18:09:29.798956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-trained AutoGluon models\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING PRE-TRAINED AUTOGLUON MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "autogluon_models = {}\n",
    "model_dir = \"models/autogluon_production\"\n",
    "\n",
    "for target in target_cols:\n",
    "    try:\n",
    "        target_model_path = os.path.join(model_dir, target)\n",
    "        print(f\"\\nüìÇ Loading {target} model from {target_model_path}...\", end=\" \")\n",
    "        predictor = TabularPredictor.load(target_model_path)\n",
    "        autogluon_models[target] = predictor\n",
    "        print(f\"‚úÖ Success! Expected features: {len(predictor.features)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        print(f\"‚ö†Ô∏è  Falling back to zero predictions for {target}\")\n",
    "        autogluon_models[target] = None\n",
    "\n",
    "all_models_loaded = all(model is not None for model in autogluon_models.values())\n",
    "if all_models_loaded:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ ALL AUTOGLUON MODELS SUCCESSFULLY LOADED!\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  WARNING: Some AutoGluon models failed to load!\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2a77e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:29.966183Z",
     "iopub.status.busy": "2025-10-31T18:09:29.965814Z",
     "iopub.status.idle": "2025-10-31T18:09:34.086580Z",
     "shell.execute_reply": "2025-10-31T18:09:34.085848Z"
    },
    "papermill": {
     "duration": 4.131239,
     "end_time": "2025-10-31T18:09:34.088999",
     "exception": false,
     "start_time": "2025-10-31T18:09:29.957760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Models are pre-trained by train_autogluon_production.py\n",
    "# No training needed here - we're just using them for inference!\n",
    "print(\"\\n‚úì Pre-trained AutoGluon models are ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b630d69",
   "metadata": {
    "papermill": {
     "duration": 0.007013,
     "end_time": "2025-10-31T18:09:34.105644",
     "exception": false,
     "start_time": "2025-10-31T18:09:34.098631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Test Predictions and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8314cb7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:34.122097Z",
     "iopub.status.busy": "2025-10-31T18:09:34.121777Z",
     "iopub.status.idle": "2025-10-31T18:09:34.141339Z",
     "shell.execute_reply": "2025-10-31T18:09:34.140260Z"
    },
    "papermill": {
     "duration": 0.02921,
     "end_time": "2025-10-31T18:09:34.142841",
     "exception": false,
     "start_time": "2025-10-31T18:09:34.113631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract comprehensive features for test data\n",
    "print(\"=\" * 70)\n",
    "print(\"EXTRACTING COMPREHENSIVE FEATURES FOR TEST DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_features_list = []\n",
    "print(\"Extracting 34 comprehensive features (simple + chemistry + RDKit)...\")\n",
    "for idx, smiles in tqdm(test_df['SMILES'].items(), total=len(test_df)):\n",
    "    try:\n",
    "        smiles_str = str(smiles) if pd.notna(smiles) else \"\"\n",
    "        features_dict = extract_comprehensive_features(smiles_str)\n",
    "        test_features_list.append(features_dict)\n",
    "    except:\n",
    "        # Fallback to zero features\n",
    "        test_features_list.append(extract_comprehensive_features(\"\"))\n",
    "\n",
    "test_features_df = pd.DataFrame(test_features_list, index=test_df.index)\n",
    "print(f\"‚úì Extracted {len(test_features_df)} feature vectors with {len(test_features_df.columns)} features\")\n",
    "print(f\"   Feature names: {list(test_features_df.columns)}\")\n",
    "\n",
    "# Handle any NaN/inf values\n",
    "test_features_df = test_features_df.fillna(0.0)\n",
    "test_features_df = test_features_df.replace([np.inf, -np.inf], 0.0)\n",
    "print(f\"‚úì Test features ready for AutoGluon prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0afeb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:34.158989Z",
     "iopub.status.busy": "2025-10-31T18:09:34.158715Z",
     "iopub.status.idle": "2025-10-31T18:09:34.172132Z",
     "shell.execute_reply": "2025-10-31T18:09:34.170884Z"
    },
    "papermill": {
     "duration": 0.023189,
     "end_time": "2025-10-31T18:09:34.173710",
     "exception": false,
     "start_time": "2025-10-31T18:09:34.150521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate predictions using AutoGluon models\n",
    "print(\"=\" * 70)\n",
    "print(\"GENERATING PREDICTIONS WITH AUTOGLUON MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "autogluon_predictions = np.zeros((len(test_features_df), len(target_cols)))\n",
    "\n",
    "for i, target in enumerate(target_cols):\n",
    "    try:\n",
    "        if autogluon_models[target] is not None:\n",
    "            predictor = autogluon_models[target]\n",
    "            print(f\"\\nü§ñ Predicting {target}...\", end=\" \")\n",
    "            \n",
    "            # Ensure features are in the right format\n",
    "            X_test_clean = test_features_df.fillna(0.0).values\n",
    "            X_test_clean = np.nan_to_num(X_test_clean, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            # AutoGluon expects DataFrame with correct feature names\n",
    "            # Create a DataFrame with test features and select only features the model knows about\n",
    "            test_input_df = test_features_df.copy()\n",
    "            \n",
    "            # Predict\n",
    "            preds = predictor.predict(test_input_df, verbose=0)\n",
    "            \n",
    "            if isinstance(preds, (pd.Series, pd.DataFrame)):\n",
    "                preds = preds.values.flatten()\n",
    "            \n",
    "            autogluon_predictions[:, i] = preds\n",
    "            pred_min, pred_max = preds.min(), preds.max()\n",
    "            pred_mean = preds.mean()\n",
    "            print(f\"‚úÖ Done! Range: [{pred_min:.4f}, {pred_max:.4f}], mean: {pred_mean:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  {target} model not loaded - using zero predictions\")\n",
    "            autogluon_predictions[:, i] = 0.0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Prediction failed for {target}: {e}\")\n",
    "        print(f\"   Using zero predictions as fallback\")\n",
    "        autogluon_predictions[:, i] = 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ PREDICTIONS COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb7c41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T18:09:34.208919Z",
     "iopub.status.busy": "2025-10-31T18:09:34.208624Z",
     "iopub.status.idle": "2025-10-31T18:09:34.251207Z",
     "shell.execute_reply": "2025-10-31T18:09:34.250087Z"
    },
    "papermill": {
     "duration": 0.053026,
     "end_time": "2025-10-31T18:09:34.252726",
     "exception": false,
     "start_time": "2025-10-31T18:09:34.199700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create submission with AutoGluon predictions\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING SUBMISSION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    submission = sample_submission.copy()\n",
    "    \n",
    "    # Ensure we have the right number of predictions\n",
    "    if len(autogluon_predictions) != len(submission):\n",
    "        print(f\"‚ö†Ô∏è  Warning: Prediction length {len(autogluon_predictions)} != submission length {len(submission)}\")\n",
    "        if len(autogluon_predictions) < len(submission):\n",
    "            padding = np.zeros((len(submission) - len(autogluon_predictions), len(target_cols)))\n",
    "            autogluon_predictions = np.vstack([autogluon_predictions, padding])\n",
    "        else:\n",
    "            autogluon_predictions = autogluon_predictions[:len(submission)]\n",
    "    \n",
    "    # Fill submission with AutoGluon predictions\n",
    "    print(\"\\nFilling submission with AutoGluon predictions...\")\n",
    "    for i, target in enumerate(target_cols):\n",
    "        submission[target] = autogluon_predictions[:, i]\n",
    "        print(f\"  {target}: {autogluon_predictions[:, i].min():.4f} to {autogluon_predictions[:, i].max():.4f}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CRITICAL: Apply Tg transformation discovered by 2nd place winner\n",
    "    # ========================================================================\n",
    "    # Analysis of winning solutions revealed that the competition was determined\n",
    "    # by a Tg (glass transition temperature) distribution shift in the test data.\n",
    "    # The 2nd place winner (Private LB: 0.066) discovered that applying a simple\n",
    "    # transformation to Tg predictions was worth 10-20x more than model complexity.\n",
    "    #\n",
    "    # Transformation: (9/5) * Tg + 45\n",
    "    # This is similar to Celsius->Fahrenheit conversion, suggesting a units/scale\n",
    "    # issue between train and test datasets for Tg specifically.\n",
    "    #\n",
    "    # Impact: A basic ExtraTreesRegressor with this transformation (0.077) performed\n",
    "    # as well as complex BERT ensembles with 1.1M external data (0.075).\n",
    "    #\n",
    "    # Reference: 2nd place solution write-up on Kaggle competition discussion\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"APPLYING TG TRANSFORMATION (2nd Place Discovery)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Original Tg range: [{submission['Tg'].min():.2f}, {submission['Tg'].max():.2f}]\")\n",
    "    print(f\"Original Tg mean: {submission['Tg'].mean():.2f}\")\n",
    "    \n",
    "    # Apply the transformation\n",
    "    submission['Tg'] = (9/5) * submission['Tg'] + 45\n",
    "    \n",
    "    print(f\"‚úÖ Transformed Tg range: [{submission['Tg'].min():.2f}, {submission['Tg'].max():.2f}]\")\n",
    "    print(f\"‚úÖ Transformed Tg mean: {submission['Tg'].mean():.2f}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Sanity checks\n",
    "    print(\"Submission validation:\")\n",
    "    print(f\"  Shape: {submission.shape}\")\n",
    "    print(f\"  Columns: {list(submission.columns)}\")\n",
    "    print(f\"  Any NaN: {submission.isnull().any().any()}\")\n",
    "    print(f\"  Any inf: {np.isinf(submission.select_dtypes(include=[np.number])).any().any()}\")\n",
    "    \n",
    "    # Replace any remaining NaN/inf values\n",
    "    submission = submission.fillna(0.0)\n",
    "    numeric_cols = submission.select_dtypes(include=[np.number]).columns\n",
    "    submission[numeric_cols] = submission[numeric_cols].replace([np.inf, -np.inf], 0.0)\n",
    "    \n",
    "    print(\"\\nüìä Submission preview:\")\n",
    "    print(submission.head(10))\n",
    "    \n",
    "    print(\"\\nüìà Submission statistics:\")\n",
    "    print(submission[target_cols].describe())\n",
    "    \n",
    "    # Save submission\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ SUBMISSION SAVED TO submission.csv!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"ü§ñ Using AutoGluon WeightedEnsemble_L2 with 34 comprehensive features\")\n",
    "    print(\"üìä Includes Tg transformation for improved leaderboard performance\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Submission creation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    # Create minimal fallback submission\n",
    "    try:\n",
    "        print(\"\\n‚ö†Ô∏è  Creating fallback submission with zeros...\")\n",
    "        submission = sample_submission.copy()\n",
    "        for target in target_cols:\n",
    "            submission[target] = 0.0\n",
    "        submission.to_csv('submission.csv', index=False)\n",
    "        print(\"‚úì Fallback submission created\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Even fallback submission failed: {e2}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cabe179",
   "metadata": {
    "papermill": {
     "duration": 0.007234,
     "end_time": "2025-10-31T18:09:34.267684",
     "exception": false,
     "start_time": "2025-10-31T18:09:34.260450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Final Summary\n",
    "\n",
    "## üéØ What Makes This Version Special\n",
    "\n",
    "### **NEW: External Tc Data Augmentation** üéâ\n",
    "Added 875+ external Tc samples to boost training data:\n",
    "- **Original:** 737 Tc samples in training\n",
    "- **Augmented:** ~1,600+ Tc samples (2.2x increase!)\n",
    "- **Impact:** More data = better predictions, especially for underrepresented properties\n",
    "- **Strategy:** Only add non-overlapping SMILES to avoid data leakage\n",
    "\n",
    "### **NEW: SMILES Canonicalization**\n",
    "Added SMILES canonicalization to standardize molecular representations:\n",
    "- Removes duplicates (e.g., `*C=C(*)C` == `*C(=C*)C`)\n",
    "- Ensures consistent feature extraction\n",
    "- Uses RDKit ONLY for canonicalization, NOT for complex features\n",
    "\n",
    "## üöÄ Optimization Stack\n",
    "\n",
    "### 1. **External Data Augmentation** (NEW!)\n",
    "- Adds 875+ external Tc samples\n",
    "- Doubles Tc training data (737 ‚Üí ~1,600)\n",
    "- Improves predictions for underrepresented properties\n",
    "- No data leakage (non-overlapping SMILES only)\n",
    "\n",
    "### 2. **SMILES Canonicalization** (NEW!)\n",
    "- Standardizes molecular representations\n",
    "- Prevents duplicate encodings\n",
    "- Uses RDKit minimally (canonicalization only)\n",
    "\n",
    "### 3. **Tg Transformation** (2nd Place Discovery)\n",
    "- Transform: `(9/5) √ó Tg + 45`\n",
    "- Impact: ~30% improvement (0.13 ‚Üí 0.09)\n",
    "- Fixes distribution shift between train/test data\n",
    "\n",
    "### 5. **MAE Objective Alignment**\n",
    "- Uses `objective='reg:absoluteerror'` in XGBoost\n",
    "- Matches competition metric (wMAE)\n",
    "- Expected additional 5-15% improvement\n",
    "\n",
    "## üîë Key Takeaways\n",
    "\n",
    "1. **Simplicity beats complexity** for small datasets\n",
    "2. **External data augmentation** significantly boosts predictions for rare properties\n",
    "3. **SMILES canonicalization** improves data quality without adding complexity\n",
    "4. **Domain knowledge** (Tg shift) matters more than model sophistication\n",
    "5. **Metric alignment** ensures we optimize what we measure"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12966160,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 7678100,
     "sourceId": 12189904,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7690162,
     "sourceId": 12207625,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7714586,
     "sourceId": 12243815,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8095802,
     "sourceId": 12804095,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8621032,
     "sourceId": 13570798,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.294319,
   "end_time": "2025-10-31T18:09:34.997666",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-31T18:09:02.703347",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
